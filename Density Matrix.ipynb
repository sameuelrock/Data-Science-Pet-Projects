{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSdQw9VEGEzBQD5LIdE/lG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In layman's terms, we are trying to create a mathematical representation of a group of words, called a sentence, using a technique called density matrix. \n","\n","This mathematical representation is used to describe the state of a system and it is common in quantum mechanics and quantum computing.\n","\n","In the code provided, we took a sample sentence and separated it into individual words called tokens. Then, we calculated the embeddings for each of the tokens. An embedding is a numeric vector that represents a word in a high-dimensional space, capturing its semantic and syntactic properties.\n","\n","Then, we calculate the similarity between the embeddings of each pair of tokens using a similarity measure called the cosine similarity.\n","\n","Finally, we store these similarity values in a matrix called a density matrix, which can be used to describe the state of the words in the sentence in a mathematical form."],"metadata":{"id":"cYLcbySKOF0Y"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ni5Kb3FCNCI6","executionInfo":{"status":"ok","timestamp":1673447088886,"user_tz":-480,"elapsed":582,"user":{"displayName":"Sameuel Rock Salazar","userId":"15418300108365731843"}},"outputId":"dda8befa-f190-473f-8621-b68fca4e4b17"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.99258333 0.98270763 0.97463185 0.96832966 0.96337534\n","  0.95941195 0.98270763 0.95618289 0.95350769 0.95125831 0.94934231]\n"," [0.99258333 1.         0.99792889 0.99461155 0.99149992 0.98882907\n","  0.98657897 0.99792889 0.98468212 0.98307207 0.98169356 0.98050276]\n"," [0.98270763 0.99792889 1.         0.99922048 0.9978158  0.99636925\n","  0.99503924 1.         0.99385869 0.99282192 0.9919125  0.99111258]\n"," [0.97463185 0.99461155 0.99922048 1.         0.99964575 0.99895352\n","  0.99819089 0.99922048 0.99745236 0.99676953 0.99614986 0.99559146]\n"," [0.96832966 0.99149992 0.9978158  0.99964575 1.         0.99981694\n","  0.99943752 0.9978158  0.99899764 0.99855404 0.99813026 0.99773518]\n"," [0.96337534 0.98882907 0.99636925 0.99895352 0.99981694 1.\n","  0.99989621 0.99636925 0.99967122 0.99939979 0.99911701 0.99883952]\n"," [0.95941195 0.98657897 0.99503924 0.99819089 0.99943752 0.99989621\n","  1.         0.99503924 0.99993688 0.99979516 0.99961863 0.99942974]\n"," [0.98270763 0.99792889 1.         0.99922048 0.9978158  0.99636925\n","  0.99503924 1.         0.99385869 0.99282192 0.9919125  0.99111258]\n"," [0.95618289 0.98468212 0.99385869 0.99745236 0.99899764 0.99967122\n","  0.99993688 0.99385869 1.         0.99995945 0.9998658  0.99974604]\n"," [0.95350769 0.98307207 0.99282192 0.99676953 0.99855404 0.99939979\n","  0.99979516 0.99282192 0.99995945 1.         0.99997278 0.99990844]\n"," [0.95125831 0.98169356 0.9919125  0.99614986 0.99813026 0.99911701\n","  0.99961863 0.9919125  0.9998658  0.99997278 1.         0.99998106]\n"," [0.94934231 0.98050276 0.99111258 0.99559146 0.99773518 0.99883952\n","  0.99942974 0.99111258 0.99974604 0.99990844 0.99998106 1.        ]]\n"]}],"source":["import numpy as np\n","\n","# Define a sample sentence\n","sentence = \"This is a sample sentence for estimating a density matrix in NLP.\"\n","\n","# Tokenize the sentence\n","tokens = [token.rstrip('.') for token in sentence.split()]\n","\n","# Define the dimension of the density matrix\n","dimension = len(tokens)\n","\n","# Define an empty density matrix\n","density_matrix = np.zeros((dimension, dimension))\n","\n","# Load the pre-trained embeddings for the tokens\n","embeddings = {\n","    \"This\": [0.1, 0.2, 0.3],\n","    \"is\": [0.2, 0.3, 0.4],\n","    \"a\": [0.3, 0.4, 0.5],\n","    \"sample\": [0.4, 0.5, 0.6],\n","    \"sentence\": [0.5, 0.6, 0.7],\n","    \"for\": [0.6, 0.7, 0.8],\n","    \"estimating\": [0.7, 0.8, 0.9],\n","    \"density\": [0.8, 0.9, 1.0],\n","    \"matrix\": [0.9, 1.0, 1.1],\n","    \"in\": [1.0, 1.1, 1.2],\n","    \"NLP\": [1.1, 1.2, 1.3]\n","}\n","\n","# Compute the cosine similarity between each pair of embeddings\n","for i in range(dimension):\n","    for j in range(dimension):\n","        density_matrix[i][j] = np.dot(embeddings[tokens[i]], embeddings[tokens[j]]) / (np.linalg.norm(embeddings[tokens[i]]) * np.linalg.norm(embeddings[tokens[j]]))\n","\n","print(density_matrix)\n"]},{"cell_type":"markdown","source":["The above code tokenize the sentence and calculate the embedding for each token, then use the embeddings to calculate the cosine similarity which is a proper similarity measure instead of dot product. \n","\n","Also the embeddings are pre-trained in this example, you may use pre-trained embeddings from other models, like Word2Vec, GloVe or BERT, or fine-tune them on a specific dataset."],"metadata":{"id":"KJTXUY_WNLuC"}},{"cell_type":"markdown","source":["# Using Word2Vec model \n"],"metadata":{"id":"r9DAZfGDO1eI"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","# Define a sample sentence, use larger data set for more efficacy\n","sentence = \"This is a sample sentence for estimating a density matrix in NLP.\"\n","\n","# Tokenize the sentence\n","tokens = sentence.split()\n","\n","# Define the dimension of the density matrix\n","dimension = len(tokens)\n","\n","# Define an empty density matrix\n","density_matrix = np.zeros((dimension, dimension))\n","\n","# Train a Word2Vec model on the tokens\n","model = Word2Vec([tokens], min_count=1, size=32)\n","\n","# Compute the cosine similarity between each pair of embeddings\n","for i in range(dimension):\n","    for j in range(dimension):\n","        density_matrix[i][j] = model.similarity(tokens[i], tokens[j])\n","\n","print(density_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5iHpcEpgO_Lw","executionInfo":{"status":"ok","timestamp":1673447376935,"user_tz":-480,"elapsed":1018,"user":{"displayName":"Sameuel Rock Salazar","userId":"15418300108365731843"}},"outputId":"148a821f-f540-495e-bd15-7ae19c617bb3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"]},{"output_type":"stream","name":"stdout","text":["[[ 1.00000000e+00 -2.11016014e-02 -2.80287027e-01 -1.98276583e-02\n","   2.17357695e-01  1.52438015e-01 -1.07877403e-02 -2.80287027e-01\n","   1.79291874e-01 -1.11502334e-01  2.58113146e-01 -1.99248195e-01]\n"," [-2.11016014e-02  1.00000000e+00 -1.29332602e-01  1.03278860e-01\n","   1.32681489e-01 -5.75513840e-02 -4.60740775e-02 -1.29332602e-01\n","  -5.41791916e-02  2.25660056e-01 -1.36783242e-01 -2.31887028e-01]\n"," [-2.80287027e-01 -1.29332602e-01  1.00000000e+00  1.00846618e-01\n","  -2.53729615e-02 -1.82917908e-01 -7.49468431e-03  1.00000000e+00\n","   3.34177226e-01  1.46304622e-01  4.63454649e-02  1.55731142e-01]\n"," [-1.98276583e-02  1.03278860e-01  1.00846618e-01  1.00000000e+00\n","  -1.16242766e-01 -2.60698110e-01  2.23004177e-01  1.00846618e-01\n","   6.92590773e-02  1.26993850e-01  1.94668069e-01 -5.53505495e-02]\n"," [ 2.17357695e-01  1.32681489e-01 -2.53729615e-02 -1.16242766e-01\n","   1.00000000e+00  3.04359607e-02 -2.44371861e-01 -2.53729615e-02\n","  -1.52822435e-02 -1.28559366e-01 -3.12271237e-01 -3.17899019e-01]\n"," [ 1.52438015e-01 -5.75513840e-02 -1.82917908e-01 -2.60698110e-01\n","   3.04359607e-02  1.00000000e+00 -7.17110932e-04 -1.82917908e-01\n","  -2.94270098e-01  1.93617493e-01  2.00420067e-01  1.36504769e-01]\n"," [-1.07877403e-02 -4.60740775e-02 -7.49468431e-03  2.23004177e-01\n","  -2.44371861e-01 -7.17110932e-04  1.00000000e+00 -7.49468431e-03\n","   1.25598162e-01 -1.99385107e-01  6.35117516e-02 -6.25815690e-02]\n"," [-2.80287027e-01 -1.29332602e-01  1.00000000e+00  1.00846618e-01\n","  -2.53729615e-02 -1.82917908e-01 -7.49468431e-03  1.00000000e+00\n","   3.34177226e-01  1.46304622e-01  4.63454649e-02  1.55731142e-01]\n"," [ 1.79291874e-01 -5.41791916e-02  3.34177226e-01  6.92590773e-02\n","  -1.52822435e-02 -2.94270098e-01  1.25598162e-01  3.34177226e-01\n","   9.99999940e-01 -2.12736875e-01 -4.21607643e-02 -6.51668310e-02]\n"," [-1.11502334e-01  2.25660056e-01  1.46304622e-01  1.26993850e-01\n","  -1.28559366e-01  1.93617493e-01 -1.99385107e-01  1.46304622e-01\n","  -2.12736875e-01  1.00000000e+00  1.33872092e-01  1.20119900e-01]\n"," [ 2.58113146e-01 -1.36783242e-01  4.63454649e-02  1.94668069e-01\n","  -3.12271237e-01  2.00420067e-01  6.35117516e-02  4.63454649e-02\n","  -4.21607643e-02  1.33872092e-01  1.00000000e+00  1.17334373e-01]\n"," [-1.99248195e-01 -2.31887028e-01  1.55731142e-01 -5.53505495e-02\n","  -3.17899019e-01  1.36504769e-01 -6.25815690e-02  1.55731142e-01\n","  -6.51668310e-02  1.20119900e-01  1.17334373e-01  1.00000000e+00]]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-d02a8d63059d>:21: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n","  density_matrix[i][j] = model.similarity(tokens[i], tokens[j])\n"]}]},{"cell_type":"markdown","source":["This code is similar to the previous example, but it uses the word2vec algorithm to train the model on the given sentence and learn the embeddings for the tokens, then we calculate the similarity by using model.similarity() which is implemented using cosine similarity.\n","\n","It is important to note that, the training set for word2vec should be significantly large for it to be effective and generalize to new words.\n","This code creates a density matrix, but it's worth noting again that density matrices are not commonly used in NLP tasks, instead other techniques like deep learning models are widely used in NLP tasks."],"metadata":{"id":"-IieiNsVPDoF"}},{"cell_type":"markdown","source":["# Modelling Lexical Ambiguity\n","\n","Density matrices can be used to model lexical ambiguity, which is the phenomenon where a word or phrase can have multiple meanings in a sentence. One approach to modeling lexical ambiguity using density matrices is to represent each word in a sentence as a density matrix, where each entry of the matrix represents the probability of the word having a certain meaning. This is known as a density-matrix based model for lexical disambiguation.\n","\n","Here is an example of how you might use a density matrix to model lexical ambiguity for a sample sentence \"I saw her duck\" which could be interpreted in two ways: \"I saw a duck belonging to her\" or \"I saw her (the person) duck (the verb)\":\n","\n"],"metadata":{"id":"FYc2B7b_Ps5J"}},{"cell_type":"code","source":["import numpy as np\n","\n","import numpy as np\n","\n","# Define a sample sentence\n","sentences = [\"I saw her duck\", \"The bank can guarantee deposits will eventually cover future tuition costs\",...]\n","\n","# Define a dictionary of meanings for each word\n","meanings = {\n","    \"I\": [\"first person singular pronoun\"],\n","    \"saw\": [\"past tense of the verb see\"],\n","    \"her\": [\"possessive pronoun\"],\n","    \"duck\": [\"a water bird\", \"verb meaning to lower one's head\"],\n","    \"The\": [\"determiner\"],\n","    \"bank\":[\"place where people keep money\", \"land alongside a body of water\"],\n","    \"can\":[\"able to\", \"container to hold something\"],\n","    \"guarantee\": [\"promise something will happen\"],\n","    \"deposits\": [\"money left in a bank account\"],\n","    \"will\": [\"used to indicate future\"],\n","    \"eventually\": [\"happening at an unspecified time\"],\n","    \"cover\": [\"provide a protective layer over something\", \"provide enough money for something\"],\n","    \"future\": [\"time yet to come\"],\n","    \"tuition\": [\"money paid for teaching\"],\n","    \"costs\": [\"expenses\"]\n","}\n","\n","\n","# Tokenize the sentence\n","tokens = sentence.split()\n","\n","# Define an empty density matrix for each token\n","density_matrices = {token: np.zeros((2, 2)) for token in tokens}\n","\n","# Fill the density matrices\n","for i, token in enumerate(tokens):\n","    for j, meaning in enumerate(meanings[token]):\n","        density_matrices[token][j][j] = 1/len(meanings[token])\n","\n","# Compute the density matrix for the sentence\n","sentence_density_matrix = np.identity(2)\n","for token in tokens:\n","    sentence_density_matrix = np.kron(sentence_density_matrix, density_matrices[token])\n","\n","print(sentence_density_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kC6E1w0LPx3E","executionInfo":{"status":"ok","timestamp":1673447880043,"user_tz":-480,"elapsed":12,"user":{"displayName":"Sameuel Rock Salazar","userId":"15418300108365731843"}},"outputId":"1fe39451-c2c2-494e-de9e-1c0de1960ed9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.5 0.  0.  ... 0.  0.  0. ]\n"," [0.  0.5 0.  ... 0.  0.  0. ]\n"," [0.  0.  0.  ... 0.  0.  0. ]\n"," ...\n"," [0.  0.  0.  ... 0.  0.  0. ]\n"," [0.  0.  0.  ... 0.  0.  0. ]\n"," [0.  0.  0.  ... 0.  0.  0. ]]\n"]}]},{"cell_type":"markdown","source":["The above code uses the Kronecker product to combine the density matrices of the individual tokens to create a density matrix for the whole sentence, representing the probabilities of the different meanings of each word. The resulting matrix is a 4x4 matrix, whose entries represent the probability of the sentence having the meanings \"I saw a duck belonging to her\" or \"I saw her (the person) duck (the verb)\".\n","\n","It is worth noting that this is a simplified example and density matrix based models for lexical disambiguation are not widely used, instead other techniques like context-based methods such as those based on word embeddings or neural network models are widely used for lexical disambiguation tasks."],"metadata":{"id":"FeAxnAPlP7nX"}},{"cell_type":"markdown","source":["# Sentimental Analysis using NLTK Package\n","\n","Density matrices can be used in the context of sentiment analysis as a way to represent the sentiment of a text in a mathematical form. One approach is to use density matrices to represent the sentiment of a text as a probability distribution over a set of sentiment classes (e.g., positive, neutral, and negative).\n","Here is an example of how you might use a density matrix to represent the sentiment of a text in Python:"],"metadata":{"id":"BtHtQ78ARb-S"}},{"cell_type":"code","source":["import numpy as np\n","import nltk\n","nltk.download(\"vader_lexicon\")\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","# Define a sample text\n","text = \"This is a great product! I highly recommend it.\"\n","\n","# Define sentiment classes\n","sentiment_classes = [\"positive\", \"neutral\", \"negative\"]\n","\n","# Tokenize the text\n","tokens = text.split()\n","\n","# Define an empty density matrix\n","density_matrix = np.zeros((len(sentiment_classes), len(sentiment_classes)))\n","\n","# Initialize the sentiment analyzer\n","sia = SentimentIntensityAnalyzer()\n","\n","# Use the sentiment analyzer to predict the sentiment of each token\n","for token in tokens:\n","    prediction = sia.polarity_scores(token)\n","    if prediction[\"compound\"] >= 0.05:\n","        density_matrix[0][0] += 1/len(tokens)\n","    elif -0.05 < prediction[\"compound\"] < 0.05:\n","        density_matrix[1][1] += 1/len(tokens)\n","    else:\n","        density_matrix[2][2] += 1/len(tokens)\n","\n","print(density_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyGvGprwRg14","executionInfo":{"status":"ok","timestamp":1673448304675,"user_tz":-480,"elapsed":1727,"user":{"displayName":"Sameuel Rock Salazar","userId":"15418300108365731843"}},"outputId":"dfe6dd46-37d2-4e9d-ddea-12f3fe6e8988"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["[[0.22222222 0.         0.        ]\n"," [0.         0.77777778 0.        ]\n"," [0.         0.         0.        ]]\n"]}]},{"cell_type":"markdown","source":["This code uses the NLTK library to perform sentiment analysis on the tokens in the text and assigns them to one of three sentiment classes: positive, neutral, and negative. The density matrix is then filled with the predictions, where each entry of the matrix represents the probability of the text having a certain sentiment.\n","\n","Here the sentiment analyzer is the `SentimentIntensityAnalyzer()` it assigns a sentiment score to a text, with a range from -1 to 1, where -1 represents negative sentiment, 1 represents positive sentiment, and 0 represents neutral sentiment. The code uses the \"compound\" score to decide which class the text belongs to , so this is just one way to classify the sentiments."],"metadata":{"id":"v89JA0pKRoNI"}}]}